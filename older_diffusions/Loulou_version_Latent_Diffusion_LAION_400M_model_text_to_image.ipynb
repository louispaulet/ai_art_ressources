{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NUmmV5ZvrPbP"
      },
      "source": [
        "## Latent Diffusion model Text-to-image synthesis, trained on the LAION-400M dataset\n",
        "### [Latent Diffusion](https://github.com/CompVis/latent-diffusion) and training the model by CompVis and the [LAION-400M dataset](https://laion.ai/laion-400-open-dataset/) by LAION.\n",
        "\n",
        "##### Colab assembled by [@multimodalart](https://twitter.com/multimodalart). If using a Colab is not for you, you can try this model out on [Hugging Face](https://huggingface.co/spaces/multimodalart/latentdiffusion)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s746SP7WRX7w"
      },
      "outputs": [],
      "source": [
        "from google.colab import output\n",
        "output.enable_custom_widget_manager()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8WEwDMKtQAr2"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi -L"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uWLsDt7wkZfU"
      },
      "source": [
        "## Save model and outputs on Google Drive? "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aJF6wP2zkWE_"
      },
      "outputs": [],
      "source": [
        "#@markdown Enable saving outputs to Google Drive to save your creations at AI/models\n",
        "save_outputs_to_google_drive = True #@param {type:\"boolean\"}\n",
        "#@markdown Enable saving models to Google Drive to avoid downloading the 6GB model every Colab instance\n",
        "save_models_to_google_drive = True #@param {type:\"boolean\"}\n",
        "\n",
        "if save_outputs_to_google_drive or save_models_to_google_drive:\n",
        "    from google.colab import drive\n",
        "    try:\n",
        "      drive.mount('/content/gdrive')\n",
        "    except:\n",
        "      save_outputs_to_google_drive = False\n",
        "      save_models_to_google_drive = False\n",
        "\n",
        "model_path = \"/content/gdrive/MyDrive/AI/models\" if save_models_to_google_drive else \"/models/ldm/text2img-large\"\n",
        "outputs_path = \"/content/gdrive/MyDrive/AI/Latent_Diffusion\" if save_outputs_to_google_drive else \"/content/outputs\"\n",
        "!mkdir -p $model_path\n",
        "!mkdir -p $outputs_path\n",
        "print(f\"Model will be stored at {model_path}\")\n",
        "print(f\"Outputs will be saved to {outputs_path}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xEVSOJ4f0B21"
      },
      "source": [
        "# Setup stuff"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pTfGS88NUyUJ"
      },
      "outputs": [],
      "source": [
        "from tqdm.auto import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NHgUAp48qwoG"
      },
      "outputs": [],
      "source": [
        "#@title Installation\n",
        "!git clone https://github.com/crowsonkb/latent-diffusion.git\n",
        "!git clone https://github.com/CompVis/taming-transformers\n",
        "!pip install -e ./taming-transformers\n",
        "!pip install omegaconf>=2.0.0 pytorch-lightning>=1.0.8 torch-fidelity einops\n",
        "!pip install transformers\n",
        "!pip install open_clip_torch\n",
        "!pip install autokeras\n",
        "!pip install tensorflow\n",
        "import sys\n",
        "sys.path.append(\".\")\n",
        "sys.path.append('./taming-transformers')\n",
        "from taming.models import vqgan "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fNqCqQDoyZmq"
      },
      "source": [
        "Now, download the checkpoint (~5.7 GB). This will usually take 3-6 minutes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "cNHvQBhzyXCI"
      },
      "outputs": [],
      "source": [
        "#@title Download model\n",
        "%cd /content/latent-diffusion\n",
        "\n",
        "import os\n",
        "if os.path.isfile(f\"{model_path}/latent_diffusion_txt2img_f8_large.ckpt\"):\n",
        "    print(\"Using saved model from Google Drive\")\n",
        "else:    \n",
        "    !wget -O $model_path/latent_diffusion_txt2img_f8_large.ckpt https://ommer-lab.com/files/latent-diffusion/nitro/txt2img-f8-large/model.ckpt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ThxmCePqt1mt"
      },
      "source": [
        "Let's also check what type of GPU we've got."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jbL2zJ7Pt7Jl"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1tWAqdwk0Nrn"
      },
      "source": [
        "Load it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fnGwQRhtyBhb"
      },
      "outputs": [],
      "source": [
        "#@title loading utils\n",
        "import torch\n",
        "from omegaconf import OmegaConf\n",
        "\n",
        "from ldm.util import instantiate_from_config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BPnyd-XUKbfE"
      },
      "outputs": [],
      "source": [
        "#@title Import stuff\n",
        "import argparse, os, sys, glob\n",
        "import torch\n",
        "import numpy as np\n",
        "from omegaconf import OmegaConf\n",
        "from PIL import Image\n",
        "from tqdm.auto import tqdm, trange\n",
        "tqdm_auto_model = __import__(\"tqdm.auto\", fromlist=[None]) \n",
        "sys.modules['tqdm'] = tqdm_auto_model\n",
        "from einops import rearrange\n",
        "from torchvision.utils import make_grid\n",
        "import transformers\n",
        "import gc\n",
        "from ldm.util import instantiate_from_config\n",
        "from ldm.models.diffusion.ddim import DDIMSampler\n",
        "from ldm.models.diffusion.plms import PLMSSampler\n",
        "from open_clip import tokenizer\n",
        "import open_clip\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HY_7vvnPThzS"
      },
      "outputs": [],
      "source": [
        "#@title Load necessary functions\n",
        "\n",
        "\n",
        "def load_safety_model(clip_model):\n",
        "    \"\"\"load the safety model\"\"\"\n",
        "    import autokeras as ak  # pylint: disable=import-outside-toplevel\n",
        "    from tensorflow.keras.models import load_model  # pylint: disable=import-outside-toplevel\n",
        "    from os.path import expanduser  # pylint: disable=import-outside-toplevel\n",
        "\n",
        "    home = expanduser(\"~\")\n",
        "\n",
        "    cache_folder = home + \"/.cache/clip_retrieval/\" + clip_model.replace(\"/\", \"_\")\n",
        "    if clip_model == \"ViT-L/14\":\n",
        "        model_dir = cache_folder + \"/clip_autokeras_binary_nsfw\"\n",
        "        dim = 768\n",
        "    elif clip_model == \"ViT-B/32\":\n",
        "        model_dir = cache_folder + \"/clip_autokeras_nsfw_b32\"\n",
        "        dim = 512\n",
        "    else:\n",
        "        raise ValueError(\"Unknown clip model\")\n",
        "    if not os.path.exists(model_dir):\n",
        "        os.makedirs(cache_folder, exist_ok=True)\n",
        "\n",
        "        from urllib.request import urlretrieve  # pylint: disable=import-outside-toplevel\n",
        "\n",
        "        path_to_zip_file = cache_folder + \"/clip_autokeras_binary_nsfw.zip\"\n",
        "        if clip_model == \"ViT-L/14\":\n",
        "            url_model = \"https://raw.githubusercontent.com/LAION-AI/CLIP-based-NSFW-Detector/main/clip_autokeras_binary_nsfw.zip\"\n",
        "        elif clip_model == \"ViT-B/32\":\n",
        "            url_model = (\n",
        "                \"https://raw.githubusercontent.com/LAION-AI/CLIP-based-NSFW-Detector/main/clip_autokeras_nsfw_b32.zip\"\n",
        "            )\n",
        "        else:\n",
        "            raise ValueError(\"Unknown model {}\".format(clip_model))\n",
        "        urlretrieve(url_model, path_to_zip_file)\n",
        "        import zipfile  # pylint: disable=import-outside-toplevel\n",
        "\n",
        "        with zipfile.ZipFile(path_to_zip_file, \"r\") as zip_ref:\n",
        "            zip_ref.extractall(cache_folder)\n",
        "\n",
        "    loaded_model = load_model(model_dir, custom_objects=ak.CUSTOM_OBJECTS)\n",
        "    loaded_model.predict(np.random.rand(10 ** 3, dim).astype(\"float32\"), batch_size=10 ** 3)\n",
        "\n",
        "    return loaded_model\n",
        "\n",
        "def is_unsafe(safety_model, embeddings, threshold=0.5):\n",
        "    \"\"\"find unsafe embeddings\"\"\"\n",
        "    nsfw_values = safety_model.predict(embeddings, batch_size=embeddings.shape[0])\n",
        "    x = np.array([e[0] for e in nsfw_values])\n",
        "    #print(x)\n",
        "    return False\n",
        "    # return True if x > threshold else False\n",
        "#NSFW CLIP Filter\n",
        "safety_model = load_safety_model(\"ViT-B/32\")\n",
        "clip_model, _, preprocess = open_clip.create_model_and_transforms('ViT-B-32', pretrained='openai')\n",
        "\n",
        "def load_model_from_config(config, ckpt, verbose=False):\n",
        "    print(f\"Loading model from {ckpt}\")\n",
        "    pl_sd = torch.load(ckpt, map_location=\"cuda:0\")\n",
        "    sd = pl_sd[\"state_dict\"]\n",
        "    model = instantiate_from_config(config.model)\n",
        "    m, u = model.load_state_dict(sd, strict=False)\n",
        "    if len(m) > 0 and verbose:\n",
        "        print(\"missing keys:\")\n",
        "        print(m)\n",
        "    if len(u) > 0 and verbose:\n",
        "        print(\"unexpected keys:\")\n",
        "        print(u)\n",
        "\n",
        "    model = model.half().cuda()\n",
        "    model.eval()\n",
        "    return model\n",
        "\n",
        "config = OmegaConf.load(\"configs/latent-diffusion/txt2img-1p4B-eval.yaml\") \n",
        "model = load_model_from_config(config, f\"{model_path}/latent_diffusion_txt2img_f8_large.ckpt\")\n",
        "\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "model = model.to(device)\n",
        "def run(opt):\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "    if opt.plms:\n",
        "        opt.ddim_eta = 0\n",
        "        sampler = PLMSSampler(model)\n",
        "    else:\n",
        "        sampler = DDIMSampler(model)\n",
        "    \n",
        "    os.makedirs(opt.outdir, exist_ok=True)\n",
        "    outpath = opt.outdir\n",
        "\n",
        "    prompt = opt.prompt\n",
        "\n",
        "\n",
        "    sample_path = os.path.join(outpath, \"samples\")\n",
        "    os.makedirs(sample_path, exist_ok=True)\n",
        "    base_count = len(os.listdir(sample_path))\n",
        "\n",
        "    all_samples=list()\n",
        "    with torch.no_grad():\n",
        "        with torch.cuda.amp.autocast():\n",
        "            with model.ema_scope():\n",
        "                uc = None\n",
        "                if opt.scale > 0:\n",
        "                    uc = model.get_learned_conditioning(opt.n_samples * [\"\"])\n",
        "                for n in trange(opt.n_iter, desc=\"Sampling\"):\n",
        "                    c = model.get_learned_conditioning(opt.n_samples * [prompt])\n",
        "                    shape = [4, opt.H//8, opt.W//8]\n",
        "                    samples_ddim, _ = sampler.sample(S=opt.ddim_steps,\n",
        "                                                    conditioning=c,\n",
        "                                                    batch_size=opt.n_samples,\n",
        "                                                    shape=shape,\n",
        "                                                    verbose=False,\n",
        "                                                    unconditional_guidance_scale=opt.scale,\n",
        "                                                    unconditional_conditioning=uc,\n",
        "                                                    eta=opt.ddim_eta)\n",
        "\n",
        "                    x_samples_ddim = model.decode_first_stage(samples_ddim)\n",
        "                    x_samples_ddim = torch.clamp((x_samples_ddim+1.0)/2.0, min=0.0, max=1.0)\n",
        "\n",
        "                    for x_sample in x_samples_ddim:\n",
        "                        x_sample = 255. * rearrange(x_sample.cpu().numpy(), 'c h w -> h w c')\n",
        "                        image_vector = Image.fromarray(x_sample.astype(np.uint8))\n",
        "                        image = preprocess(image_vector).unsqueeze(0)\n",
        "                        with torch.no_grad():\n",
        "                          image_features = clip_model.encode_image(image)\n",
        "                        image_features /= image_features.norm(dim=-1, keepdim=True)\n",
        "                        query = image_features.cpu().detach().numpy().astype(\"float32\")\n",
        "                        unsafe = is_unsafe(safety_model,query,opt.nsfw_threshold)\n",
        "                        if(not unsafe):\n",
        "                          image_vector.save(os.path.join(sample_path, f\"{base_count:04}.png\"))\n",
        "                        else:\n",
        "                          raise Exception('Potential NSFW content was detected on your outputs. Try again with different prompts. If you feel your prompt was not supposed to give NSFW outputs, this may be due to a bias in the model')\n",
        "                        base_count += 1\n",
        "                    all_samples.append(x_samples_ddim)\n",
        "\n",
        "\n",
        "    # additionally, save as grid\n",
        "    grid = torch.stack(all_samples, 0)\n",
        "    grid = rearrange(grid, 'n b c h w -> (n b) c h w')\n",
        "    grid = make_grid(grid, nrow=opt.n_samples)\n",
        "\n",
        "    # to image\n",
        "    grid = 255. * rearrange(grid, 'c h w -> h w c').cpu().numpy()\n",
        "    \n",
        "    Image.fromarray(grid.astype(np.uint8)).save(os.path.join(outpath, f'{prompt.replace(\" \", \"-\")}.png'))\n",
        "    display(Image.fromarray(grid.astype(np.uint8)))\n",
        "    #print(f\"Your samples are ready and waiting four you here: \\n{outpath} \\nEnjoy.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DNfjrKeONV9z"
      },
      "source": [
        "# OPTION : Archive old runs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AOWuW5n2O6SC"
      },
      "outputs": [],
      "source": [
        "!mkdir /content/gdrive/MyDrive/AI/Latent_Diffusion/$(date +%Y%m%d_%H%M%S) && mv /content/gdrive/MyDrive/AI/Latent_Diffusion/*.png /content/gdrive/MyDrive/AI/Latent_Diffusion/$(date +%Y%m%d_%H%M%S)/ && mv /content/gdrive/MyDrive/AI/Latent_Diffusion/samples /content/gdrive/MyDrive/AI/Latent_Diffusion/$(date +%Y%m%d_%H%M%S)/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z-J8eQDa0Kam"
      },
      "source": [
        "# First run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "fmafGmcyT1mZ"
      },
      "outputs": [],
      "source": [
        "#@title Parameters\n",
        "import argparse\n",
        "Prompt = \"stylized hamster logo on a white background\" #@param{type:\"string\"}\n",
        "Steps = 50 #@param {type:\"integer\"}\n",
        "ETA = 0.5 #@param{type:\"number\"}\n",
        "Iterations =  10#@param{type:\"integer\"}\n",
        "Width=256 #@param{type:\"integer\"}\n",
        "Height=256 #@param{type:\"integer\"}\n",
        "Samples_in_parallel=10 #@param{type:\"integer\"}\n",
        "Diversity_scale=5 #@param {type:\"number\"}\n",
        "PLMS_sampling=True #@param {type:\"boolean\"}\n",
        "\n",
        "args = argparse.Namespace(\n",
        "    prompt = Prompt, \n",
        "    outdir=f'{outputs_path}',\n",
        "    ddim_steps = Steps,\n",
        "    ddim_eta = ETA,\n",
        "    n_iter = Iterations,\n",
        "    W=Width,\n",
        "    H=Height,\n",
        "    n_samples=Samples_in_parallel,\n",
        "    scale=Diversity_scale,\n",
        "    plms=PLMS_sampling,\n",
        "    nsfw_threshold=0.5\n",
        ")\n",
        "run(args)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "PeRCNa_yAKxN"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "f_P-hs2IAK9l"
      },
      "outputs": [],
      "source": [
        "#@title Parameters\n",
        "import argparse\n",
        "Prompt = \"minimalist hamster logo on a white background\" #@param{type:\"string\"}\n",
        "Steps = 50 #@param {type:\"integer\"}\n",
        "ETA = 0.5 #@param{type:\"number\"}\n",
        "Iterations =  10#@param{type:\"integer\"}\n",
        "Width=256 #@param{type:\"integer\"}\n",
        "Height=256 #@param{type:\"integer\"}\n",
        "Samples_in_parallel=10 #@param{type:\"integer\"}\n",
        "Diversity_scale=5 #@param {type:\"number\"}\n",
        "PLMS_sampling=True #@param {type:\"boolean\"}\n",
        "\n",
        "args = argparse.Namespace(\n",
        "    prompt = Prompt, \n",
        "    outdir=f'{outputs_path}',\n",
        "    ddim_steps = Steps,\n",
        "    ddim_eta = ETA,\n",
        "    n_iter = Iterations,\n",
        "    W=Width,\n",
        "    H=Height,\n",
        "    n_samples=Samples_in_parallel,\n",
        "    scale=Diversity_scale,\n",
        "    plms=PLMS_sampling,\n",
        "    nsfw_threshold=0.5\n",
        ")\n",
        "run(args)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "provenance": [],
      "private_outputs": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}